{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1pCkQHA6q36",
        "outputId": "da1b8d27-b3a6-4b26-ed75-6da8ecde2307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"YOUR_KEY\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBxBsUtv6t88",
        "outputId": "631ebe0a-ea9c-4a6a-9cdb-9d583683171f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY6ozt2-80x_",
        "outputId": "d8073ab3-e229-4351-fbca-16cc78d4ef2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-1.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.7.17-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.5)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting reportlab\n",
            "  Downloading reportlab-4.4.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.4.59)\n",
            "Collecting groq<1.0.0,>=0.30.0 (from langchain-groq)\n",
            "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (1.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from tavily-python) (2.32.4)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.12.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.28.1)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.4)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith) (3.11.5)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langsmith) (25.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.12.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (1.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-groq) (9.1.2)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (2.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.5.1->tavily-python) (2025.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Downloading langchain_groq-1.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading tavily_python-0.7.17-py3-none-any.whl (18 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading reportlab-4.4.7-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab, PyPDF2, tavily-python, groq, langchain-groq\n",
            "Successfully installed PyPDF2-3.0.1 groq-0.37.1 langchain-groq-1.1.1 reportlab-4.4.7 tavily-python-0.7.17\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-groq tavily-python langgraph PyPDF2 reportlab sentence-transformers langsmith\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"YOUR_KEY\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"YOUR_KEY\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"YOUR_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "%%writefile backend.py\n",
        "\n",
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
        "import os, re, json, datetime, html, time, hashlib\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, TypedDict\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from IPython.display import Markdown, display\n",
        "os.environ['EMBEDDING_DEBUG'] = '0'\n",
        "\n",
        "# Global buffer for ALL raw LLM logs\n",
        "\n",
        "GLOBAL_RAW_LOG: List[str] = []\n",
        "\n",
        "#  LangSmith tracing\n",
        "\n",
        "try:\n",
        "    from langsmith import traceable\n",
        "except ImportError:\n",
        "    # No-op decorator if langsmith not installed\n",
        "    def traceable(*targs, **tkwargs):\n",
        "        def decorator(fn):\n",
        "            return fn\n",
        "        return decorator\n",
        "\n",
        "# API Keys (Groq + Tavily)\n",
        "\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"\").strip()\n",
        "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\", \"\").strip()\n",
        "\n",
        "if not GROQ_API_KEY or not TAVILY_API_KEY:\n",
        "    raise SystemExit(\n",
        "        \"ERROR: GROQ_API_KEY and TAVILY_API_KEY must be set in environment.\\n\"\n",
        "        \"In Colab, run:\\n\"\n",
        "        \"import os\\n\"\n",
        "        \"os.environ['GROQ_API_KEY'] = 'your_groq_key'\\n\"\n",
        "        \"os.environ['TAVILY_API_KEY'] = 'your_tavily_key'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# External clients & PDF lib (reading)\n",
        "\n",
        "try:\n",
        "    from tavily import TavilyClient\n",
        "except Exception:\n",
        "    raise RuntimeError(\"Missing tavily library. Install with: pip install tavily-python\")\n",
        "\n",
        "# PDF lib for reading user PDFs\n",
        "_pdf_lib = None\n",
        "if importlib.util.find_spec(\"PyPDF2\"):\n",
        "    import PyPDF2 as _pdf_lib\n",
        "else:\n",
        "    _pdf_lib = None  # PDF ingestion will error if not installed\n",
        "\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "\n",
        "\n",
        "# PDF writer for saving raw LLM outputs\n",
        "\n",
        "try:\n",
        "    from reportlab.pdfgen import canvas\n",
        "    from reportlab.lib.pagesizes import letter\n",
        "    _pdf_writer_available = True\n",
        "except ImportError:\n",
        "    _pdf_writer_available = False\n",
        "\n",
        "\n",
        "def save_raw_to_pdf(text: str, filename: str):\n",
        "    if not _pdf_writer_available:\n",
        "        print(\"[PDF] reportlab not installed; skipping PDF save.\")\n",
        "        return\n",
        "    try:\n",
        "        c = canvas.Canvas(filename, pagesize=letter)\n",
        "        width, height = letter\n",
        "        x_margin = 40\n",
        "        y = height - 50\n",
        "        max_width_chars = 110\n",
        "        for line in text.split(\"\\n\"):\n",
        "            while len(line) > max_width_chars:\n",
        "                chunk = line[:max_width_chars]\n",
        "                line = line[max_width_chars:]\n",
        "                if y < 40:\n",
        "                    c.showPage()\n",
        "                    y = height - 50\n",
        "                c.drawString(x_margin, y, chunk)\n",
        "                y -= 15\n",
        "            if y < 40:\n",
        "                c.showPage()\n",
        "                y = height - 50\n",
        "            c.drawString(x_margin, y, line)\n",
        "            y -= 15\n",
        "        c.save()\n",
        "        print(f\"[PDF Saved] {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[PDF ERROR] Failed to save PDF '{filename}': {e}\")\n",
        "\n",
        "\n",
        "def save_all_raw_to_one_pdf(filename: str = \"all_llm_raw_output.pdf\"):\n",
        "    if not GLOBAL_RAW_LOG:\n",
        "        print(\"[PDF] No raw LLM data to save.\")\n",
        "        return\n",
        "    full_text = \"\\n\".join(GLOBAL_RAW_LOG)\n",
        "    save_raw_to_pdf(full_text, filename)\n",
        "\n",
        "\n",
        "\n",
        "# Groq LLM via LangChain\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "GROQ_MODEL = os.getenv(\"GROQ_MODEL\", \"llama-3.1-8b-instant\")\n",
        "_LLM_CACHE_PATH = Path(\"/tmp/groq_inference_cache.json\")\n",
        "try:\n",
        "    _LLM_CACHE = json.load(open(_LLM_CACHE_PATH)) if _LLM_CACHE_PATH.exists() else {}\n",
        "except Exception:\n",
        "    _LLM_CACHE = {}\n",
        "\n",
        "\n",
        "def _save_llm_cache():\n",
        "    try:\n",
        "        json.dump(_LLM_CACHE, open(_LLM_CACHE_PATH, \"w\"), indent=2, ensure_ascii=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def _prompt_key(prompt: str) -> str:\n",
        "    return hashlib.sha256(prompt.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "\n",
        "groq_llm = ChatGroq(\n",
        "    model=GROQ_MODEL,\n",
        "    temperature=0.3,\n",
        "    max_tokens=800,\n",
        "    groq_api_key=GROQ_API_KEY,\n",
        ")\n",
        "\n",
        "\n",
        "@traceable(name=\"groq_llm_call\")\n",
        "def groq_chat(\n",
        "    prompt: str,\n",
        "    *,\n",
        "    use_cache: bool = True,\n",
        "    max_retries: int = 3,\n",
        "    max_new_tokens: int = 800,\n",
        "    timeout: int = 60,\n",
        ") -> str:\n",
        "\n",
        "    key = _prompt_key(prompt)\n",
        "\n",
        "    # ‚úÖ Optional cache usage\n",
        "    if use_cache and key in _LLM_CACHE:\n",
        "        txt = _LLM_CACHE[key]\n",
        "        GLOBAL_RAW_LOG.append(\n",
        "            f\"\\n\\n=== CACHED LLM CALL ===\\n\"\n",
        "            f\"Timestamp: {datetime.datetime.now(datetime.timezone.utc).isoformat()}\\n\"\n",
        "            f\"Prompt:\\n{prompt}\\n\\nResponse:\\n{txt}\\n\"\n",
        "        )\n",
        "        return txt\n",
        "\n",
        "    # ---- real LLM call ----\n",
        "    try:\n",
        "        resp = groq_llm.invoke(prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"Groq LLM error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    if hasattr(resp, \"content\"):\n",
        "        if isinstance(resp.content, str):\n",
        "            txt = resp.content\n",
        "        elif isinstance(resp.content, list):\n",
        "            txt = \"\".join(str(p) for p in resp.content)\n",
        "        else:\n",
        "            txt = str(resp.content)\n",
        "    else:\n",
        "        txt = str(resp)\n",
        "\n",
        "    txt = (txt or \"\").strip()\n",
        "\n",
        "    # ‚úÖ Save only if cache is enabled\n",
        "    if use_cache:\n",
        "        _LLM_CACHE[key] = txt\n",
        "        _save_llm_cache()\n",
        "\n",
        "    GLOBAL_RAW_LOG.append(\n",
        "        f\"\\n\\n=== RAW LLM CALL ===\\n\"\n",
        "        f\"Timestamp: {datetime.datetime.now(datetime.timezone.utc).isoformat()}\\n\"\n",
        "        f\"Prompt:\\n{prompt}\\n\\nResponse:\\n{txt}\\n\"\n",
        "    )\n",
        "\n",
        "    return txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Checkpoint structure\n",
        "\n",
        "@dataclass\n",
        "class Checkpoint:\n",
        "    id: str\n",
        "    topic: str\n",
        "    objectives: List[str]\n",
        "    success_criteria: str\n",
        "\n",
        "\n",
        "CHECKPOINTS = [\n",
        "    Checkpoint(\n",
        "        id=\"cp1\",\n",
        "        topic=\"Basics of Neural Networks\",\n",
        "        objectives=[\n",
        "            \"Understand what an artificial neuron is\",\n",
        "            \"Understand input, hidden, and output layers\",\n",
        "            \"Understand the concept of forward propagation\",\n",
        "        ],\n",
        "        success_criteria=\"Learner can explain a simple feedforward neural network.\",\n",
        "    ),\n",
        "    Checkpoint(\n",
        "        id=\"cp2\",\n",
        "        topic=\"Gradient Descent\",\n",
        "        objectives=[\n",
        "            \"Understand loss minimization\",\n",
        "            \"Understand gradient as slope of loss\",\n",
        "            \"Understand iterative parameter updates\",\n",
        "        ],\n",
        "        success_criteria=\"Learner can describe how gradient descent updates parameters.\",\n",
        "    ),\n",
        "    Checkpoint(\n",
        "        id=\"cp3\",\n",
        "        topic=\"Activation Functions\",\n",
        "        objectives=[\n",
        "            \"Know common activations (sigmoid, tanh, relu)\",\n",
        "            \"Understand when/why to use each\",\n",
        "        ],\n",
        "        success_criteria=\"Learner can choose and justify an activation for a simple task.\",\n",
        "    ),\n",
        "    Checkpoint(\n",
        "        id=\"cp4\",\n",
        "        topic=\"Backpropagation\",\n",
        "        objectives=[\n",
        "            \"Understand chain rule for gradients\",\n",
        "            \"Understand how weight updates propagate backward\",\n",
        "        ],\n",
        "        success_criteria=\"Learner can explain backpropagation at high level.\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Agent State\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    cp_id: str\n",
        "    checkpoint: Optional[Checkpoint]\n",
        "    user_notes: str\n",
        "    user_pdfs: List[str]\n",
        "    gathered_context: str\n",
        "    context_sources: List[str]\n",
        "    relevance_score_model: Optional[int]\n",
        "    refetch_attempted: bool\n",
        "    score_meta: Optional[dict]\n",
        "\n",
        "    processed_chunks: List[str]\n",
        "    questions: List[str]\n",
        "    learner_answers: List[str]\n",
        "\n",
        "    score_percent: Optional[float]\n",
        "    pass_threshold_met: Optional[bool]\n",
        "\n",
        "    question_scores: Optional[List[dict]]   # ‚úÖ ADD THIS\n",
        "\n",
        "    temp_vector_store: Optional[dict]\n",
        "    feynman_explanation: Optional[str]\n",
        "    feynman_rounds: int\n",
        "    focus_concepts: Optional[List[str]]\n",
        "    failed_objectives: List[str]\n",
        "\n",
        "\n",
        "_score_re = re.compile(r\"\\b([1-5])\\b\")\n",
        "\n",
        "\n",
        "def parse_score_from_text(raw: str) -> int:\n",
        "    if not raw:\n",
        "        return 3\n",
        "    m = _score_re.search(raw)\n",
        "    return int(m.group(1)) if m else 3\n",
        "\n",
        "\n",
        "\n",
        "# PDF extraction helpers\n",
        "\n",
        "def extract_text_from_pdf(path: str) -> str:\n",
        "    if not _pdf_lib:\n",
        "        raise RuntimeError(\"PyPDF2 not installed. `pip install PyPDF2` to enable PDF ingestion.\")\n",
        "    text = []\n",
        "    with open(path, \"rb\") as f:\n",
        "        reader = _pdf_lib.PdfReader(f)\n",
        "        for p in reader.pages:\n",
        "            try:\n",
        "                page_text = p.extract_text() or \"\"\n",
        "            except Exception:\n",
        "                page_text = \"\"\n",
        "            if page_text:\n",
        "                text.append(page_text)\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "\n",
        "def gather_texts_from_pdfs(paths: List[str]) -> str:\n",
        "    out = \"\"\n",
        "    for p in paths:\n",
        "        try:\n",
        "            t = extract_text_from_pdf(p)\n",
        "            if t.strip():\n",
        "                out += f\"\\n--- PDF: {os.path.basename(p)} ---\\n{t}\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to read PDF {p}: {e}\")\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "# Summarizer (uses Groq)\n",
        "\n",
        "def summarize_text(text: str) -> str:\n",
        "    if not text.strip():\n",
        "        return text\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following text into a focused explanation matching the learning objectives.\n",
        "Keep it concise, clean, and relevant.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text[:5000]}\\\"\\\"\\\"\\n\n",
        "Summary:\n",
        "\"\"\"\n",
        "    summary = groq_chat(prompt)\n",
        "    return summary.strip() or text[:5000]\n",
        "\n",
        "\n",
        "\n",
        "# Tavily wrapper with rate limiting (< 10 searches/min)\n",
        "\n",
        "_search_timestamps: List[float] = []\n",
        "\n",
        "\n",
        "def _enforce_search_rate_limit():\n",
        "    global _search_timestamps\n",
        "    now = time.time()\n",
        "    _search_timestamps = [t for t in _search_timestamps if now - t < 60]\n",
        "    if len(_search_timestamps) >= 9:\n",
        "        oldest = min(_search_timestamps)\n",
        "        wait = 60 - (now - oldest)\n",
        "        if wait > 0:\n",
        "            print(f\"[Rate Limit] Tavily search rate reached. Waiting {wait:.1f}s to stay under 10/min...\")\n",
        "            time.sleep(wait)\n",
        "        now = time.time()\n",
        "        _search_timestamps = [t for t in _search_timestamps if now - t < 60]\n",
        "    _search_timestamps.append(time.time())\n",
        "\n",
        "\n",
        "@traceable(name=\"tavily_search\")\n",
        "def search_tavily(query: str, max_results: int = 5) -> List[dict]:\n",
        "    _enforce_search_rate_limit()\n",
        "    try:\n",
        "        res = tavily_client.search(query=query, max_results=max_results)\n",
        "        return res.get(\"results\", []) if isinstance(res, dict) else []\n",
        "    except Exception as e:\n",
        "        print(\"Tavily search failed:\", e)\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "# Evidence cleaning & user JSON helpers\n",
        "\n",
        "def clean_evidence(raw: str) -> str:\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    s = raw\n",
        "    s = re.sub(r\"```.*?```\", \"\", s, flags=re.DOTALL)\n",
        "    s = re.sub(r\"\\s*\\\\n\\s*\", \" \", s)\n",
        "    s = re.sub(r'^\\s*\\{.*?\"evidence\"\\s*:\\s*', \"\", s, flags=re.DOTALL)\n",
        "    s = s.replace('\"covered\":', \"\")\n",
        "    s = s.replace(\"{\", \"\").replace(\"}\", \"\")\n",
        "    s = s.replace('\"\"\"', \"\").replace(\"'''\", \"\")\n",
        "    s = s.strip()\n",
        "    s = re.sub(r'^\\s*[\"\\']?evidence[\"\\']?\\s*[:\\-]?\\s*', \"\", s, flags=re.I)\n",
        "    s = html.unescape(s).strip()\n",
        "    if len(s) > 300:\n",
        "        s = s[:300].rsplit(\" \", 1)[0] + \"...\"\n",
        "    return s\n",
        "\n",
        "\n",
        "def simplify_score_meta_for_user(score_meta: Optional[dict]):\n",
        "    if not score_meta:\n",
        "        return None\n",
        "    covered = score_meta.get(\"covered_count\", 0)\n",
        "    total = score_meta.get(\"total\", 1)\n",
        "    objectives = []\n",
        "    for d in score_meta.get(\"details\", []):\n",
        "        objectives.append(\n",
        "            {\n",
        "                \"objective\": d.get(\"objective\"),\n",
        "                \"covered\": True if str(d.get(\"covered\", \"no\")).lower() == \"yes\" else False,\n",
        "                \"evidence\": clean_evidence(d.get(\"evidence\", \"\")),\n",
        "            }\n",
        "        )\n",
        "    coverage_percent = int(round((covered / total) * 100))\n",
        "    summary = f\"{covered}/{total} objectives covered\"\n",
        "    explain = f\"{coverage_percent}% ‚Äî {summary}\"\n",
        "    return {\n",
        "        \"coverage_percent\": coverage_percent,\n",
        "        \"summary\": summary,\n",
        "        \"explain\": explain,\n",
        "        \"objective_reports\": objectives,\n",
        "    }\n",
        "\n",
        "\n",
        "# Embeddings + Temporary in-memory vector store (Milestone 2)\n",
        "\n",
        "_emb_model = None\n",
        "_np = None\n",
        "_EMBEDDING_DEBUG = os.getenv(\"EMBEDDING_DEBUG\", \"0\") in (\"1\", \"true\", \"True\")\n",
        "\n",
        "# Chunking configuration\n",
        "CHUNK_SIZE = 1200\n",
        "CHUNK_OVERLAP = 250\n",
        "MIN_CHUNK_LENGTH = 300\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import numpy as np\n",
        "\n",
        "    try:\n",
        "        print(\"[Embedding] Loading SentenceTransformer 'all-MiniLM-L6-v2' ... (this may take a few seconds)\")\n",
        "        _emb_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        _np = np\n",
        "        print(\"[Embedding] Model loaded:all-MiniLM-L6-v2\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Embedding] Failed to load SentenceTransformer model: {e}\")\n",
        "        _emb_model = None\n",
        "        _np = None\n",
        "except Exception:\n",
        "    _emb_model = None\n",
        "    try:\n",
        "        import numpy as np\n",
        "        _np = np\n",
        "    except Exception:\n",
        "        _np = None\n",
        "\n",
        "\n",
        "def is_context_relevant_semantically(\n",
        "    context: str,\n",
        "    cp,\n",
        "    threshold: float = 0.35\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Embedding-based semantic relevance check.\n",
        "    Returns False if context is unrelated to checkpoint topic/objectives.\n",
        "    \"\"\"\n",
        "    if not context.strip():\n",
        "        return False\n",
        "\n",
        "    # If embeddings unavailable, don't block pipeline\n",
        "    if _emb_model is None or _np is None:\n",
        "        return True\n",
        "\n",
        "    reference_text = cp.topic + \" \" + \" \".join(cp.objectives)\n",
        "\n",
        "    try:\n",
        "        ctx_vec = _emb_model.encode([context[:2000]], convert_to_numpy=True)\n",
        "        ref_vec = _emb_model.encode([reference_text], convert_to_numpy=True)\n",
        "\n",
        "        ctx_vec = ctx_vec / (_np.linalg.norm(ctx_vec, axis=1, keepdims=True) + 1e-12)\n",
        "        ref_vec = ref_vec / (_np.linalg.norm(ref_vec, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "        similarity = float(ctx_vec @ ref_vec.T)\n",
        "\n",
        "        if _EMBEDDING_DEBUG:\n",
        "            print(f\"[Semantic Relevance] similarity={similarity:.3f}\")\n",
        "\n",
        "        return similarity >= threshold\n",
        "\n",
        "    except Exception:\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_temp_vector_store(chunks: List[str]):\n",
        "    \"\"\"\n",
        "    Build temporary in-memory vector store for this session.\n",
        "    Returns dict: { 'chunks': [...], 'vectors': np.array or None, 'meta': {...} }\n",
        "    \"\"\"\n",
        "    if not chunks:\n",
        "        return {\"chunks\": [], \"vectors\": None, \"meta\": {\"embeddings_used\": False}}\n",
        "    if _emb_model and _np is not None:\n",
        "        try:\n",
        "            vecs = _emb_model.encode(chunks, convert_to_numpy=True, show_progress_bar=False)\n",
        "            # normalize vectors for cosine sim\n",
        "            norms = _np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12\n",
        "            vecs = vecs / norms\n",
        "            store = {\"chunks\": chunks, \"vectors\": vecs, \"meta\": {\"embeddings_used\": True}}\n",
        "            if _EMBEDDING_DEBUG:\n",
        "                print(f\"[Embedding] Built vector store: vectors shape = {vecs.shape}\")\n",
        "            return store\n",
        "        except Exception as e:\n",
        "            print(f\"[Embedding] Exception while encoding chunks: {e}\")\n",
        "            pass\n",
        "    # fallback: no embeddings (vectors=None) ‚Äî token-overlap will be used\n",
        "    if _EMBEDDING_DEBUG:\n",
        "        print(\"[Embedding] No embedding model available; using token-overlap fallback.\")\n",
        "    return {\"chunks\": chunks, \"vectors\": None, \"meta\": {\"embeddings_used\": False}}\n",
        "\n",
        "\n",
        "def embedding_debug_print(store, label: str = \"\"):\n",
        "    try:\n",
        "        emb_used = bool(store and store.get(\"vectors\") is not None)\n",
        "        if emb_used:\n",
        "            vecs = store[\"vectors\"]\n",
        "            shape = getattr(vecs, \"shape\", None)\n",
        "            print(f\"[Embedding Confirm] {label} embeddings used: True | vector shape: {shape}\")\n",
        "            # print a tiny sample: first vector first 6 values\n",
        "            sample = vecs[0][:6].tolist() if hasattr(vecs[0], \"tolist\") else list(vecs[0][:6])\n",
        "            print(f\"[Embedding Confirm] sample vec[0][:6] ~ {sample}\")\n",
        "        else:\n",
        "            print(f\"[Embedding Confirm] {label} embeddings used: False\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Embedding Confirm] Error during debug print: {e}\")\n",
        "\n",
        "\n",
        "def retrieve_top_k(store, query: str, k: int = 3) -> List[str]:\n",
        "    chunks = store.get(\"chunks\", []) or []\n",
        "    vectors = store.get(\"vectors\", None)\n",
        "\n",
        "    if not chunks:\n",
        "        return []\n",
        "\n",
        "    if vectors is not None and _emb_model is not None and _np is not None:\n",
        "        try:\n",
        "            qv = _emb_model.encode([query], convert_to_numpy=True, show_progress_bar=False)\n",
        "            qv = qv / (_np.linalg.norm(qv, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "            sims = vectors @ qv.T\n",
        "            sims = sims.reshape(-1)   # ‚úÖ CRITICAL FIX\n",
        "\n",
        "            idx_sorted = sims.argsort()[::-1][:k]\n",
        "            top_chunks = [chunks[i] for i in idx_sorted if i < len(chunks)]\n",
        "\n",
        "            if _EMBEDDING_DEBUG:\n",
        "                top_info = [(int(i), float(sims[i])) for i in idx_sorted]\n",
        "                print(f\"[Retrieve Debug] top-k indices+sim: {top_info}\")\n",
        "\n",
        "            return top_chunks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[Retrieve] Embedding retrieval failed: {e}\")\n",
        "\n",
        "    # ---- fallback: token overlap ----\n",
        "    q_words = set(re.findall(r\"\\w+\", query.lower()))\n",
        "    scored = []\n",
        "    for i, c in enumerate(chunks):\n",
        "        c_words = set(re.findall(r\"\\w+\", c.lower()))\n",
        "        scored.append((len(q_words & c_words), i))\n",
        "\n",
        "    scored.sort(reverse=True)\n",
        "    return [chunks[i] for s, i in scored[:k] if s > 0] or chunks[:k]\n",
        "\n",
        "    if top:\n",
        "        if _EMBEDDING_DEBUG:\n",
        "            print(f\"[Retrieve Debug] token-overlap scores (top): {scored[:k]}\")\n",
        "        return top\n",
        "    # last fallback: first k chunks\n",
        "    return chunks[:k]\n",
        "\n",
        "\n",
        "\n",
        "# LangGraph Nodes\n",
        "\n",
        "def get_checkpoint_by_id(cp_id: str) -> Checkpoint:\n",
        "    for cp in CHECKPOINTS:\n",
        "        if cp.id == cp_id:\n",
        "            return cp\n",
        "    raise ValueError(f\"Checkpoint {cp_id} not found\")\n",
        "\n",
        "\n",
        "def start_checkpoint(state: AgentState) -> AgentState:\n",
        "    cp = get_checkpoint_by_id(state[\"cp_id\"])\n",
        "    state = dict(state)\n",
        "    state[\"checkpoint\"] = cp\n",
        "    state[\"gathered_context\"] = \"\"\n",
        "    state[\"context_sources\"] = []\n",
        "    state[\"score_meta\"] = None\n",
        "    state[\"processed_chunks\"] = []\n",
        "    state[\"questions\"] = []\n",
        "    state[\"score_percent\"] = None\n",
        "    state[\"pass_threshold_met\"] = None\n",
        "    state[\"temp_vector_store\"] = None\n",
        "    state[\"feynman_rounds\"] = 0\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "@traceable(name=\"gather_context_node\")\n",
        "def gather_context(state: AgentState) -> AgentState:\n",
        "    cp = state[\"checkpoint\"]\n",
        "    notes = state[\"user_notes\"]\n",
        "    pdfs = state.get(\"user_pdfs\", [])\n",
        "    context = \"\"\n",
        "    sources = []\n",
        "    if notes.strip():\n",
        "        context += f\"User Notes:\\n{notes.strip()}\\n\"\n",
        "        sources.append(\"user_notes\")\n",
        "    if pdfs:\n",
        "        pdf_text = gather_texts_from_pdfs(pdfs)\n",
        "        if pdf_text.strip():\n",
        "            context += pdf_text\n",
        "            sources.append(\"pdf_upload\")\n",
        "    if not context.strip():\n",
        "        query = f\"{cp.topic} - \" + \"; \".join(cp.objectives)\n",
        "        results = search_tavily(query=query)\n",
        "        for item in results:\n",
        "            content = item.get(\"content\")\n",
        "            if content:\n",
        "                context += content + \"\\n\"\n",
        "        if context.strip():\n",
        "            sources.append(\"web_search\")\n",
        "            context = summarize_text(context)\n",
        "    if context.strip() and len(context) > 5000:\n",
        "        context = summarize_text(context)\n",
        "    state = dict(state)\n",
        "    state[\"gathered_context\"] = context\n",
        "    state[\"context_sources\"] = sources\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"[Gathered Context] Source(s): {', '.join(sources) if sources else 'None'}\")\n",
        "    print(\"-\"*80)\n",
        "    print(context.strip() if context.strip() else \"[No context gathered]\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "@traceable(name=\"validate_context_node\")\n",
        "def validate_context(state: AgentState) -> AgentState:\n",
        "    cp = state[\"checkpoint\"]\n",
        "    context = state[\"gathered_context\"]\n",
        "    refetch = state[\"refetch_attempted\"]\n",
        "\n",
        "    #  SEMANTIC FILTER\n",
        "    is_relevant = is_context_relevant_semantically(context, cp)\n",
        "\n",
        "    if not is_relevant and not refetch:\n",
        "        print(\"[Semantic Filter] Context is unrelated. Refetching from web...\")\n",
        "\n",
        "        query = f\"{cp.topic} for beginners; \" + \"; \".join(cp.objectives)\n",
        "        results = search_tavily(query=query)\n",
        "\n",
        "        new_context = \"\"\n",
        "        for item in results:\n",
        "            c = item.get(\"content\")\n",
        "            if c:\n",
        "                new_context += c + \"\\n\"\n",
        "\n",
        "        if new_context.strip():\n",
        "            new_context = summarize_text(new_context)\n",
        "\n",
        "        context = new_context\n",
        "        refetch = True\n",
        "\n",
        "\n",
        "    def score_ctx_objectives_only(ctx: str):\n",
        "        objectives = cp.objectives\n",
        "        covered = 0\n",
        "        details = []\n",
        "\n",
        "        for obj in objectives:\n",
        "            prompt = f\"\"\"\n",
        "You must respond with a single valid JSON object and NOTHING else.\n",
        "\n",
        "Keys:\n",
        "- \"covered\": either \"yes\" or \"no\"\n",
        "- \"evidence\": one short sentence (<=30 words) quoting or paraphrasing the CONTEXT\n",
        "\n",
        "OBJECTIVE:\n",
        "\\\"\\\"\\\"{obj}\\\"\\\"\\\"\\n\n",
        "CONTEXT:\n",
        "\\\"\\\"\\\"{ctx[:8000]}\\\"\\\"\\\"\\n\n",
        "Return only JSON like: {{ \"covered\": \"yes\", \"evidence\": \"...\" }}\n",
        "\"\"\"\n",
        "            raw = groq_chat(prompt).strip()\n",
        "\n",
        "            cov = \"no\"\n",
        "            evidence = \"\"\n",
        "\n",
        "            try:\n",
        "                import json as _json, re as _re\n",
        "                m = _re.search(r\"\\{.*\\}\", raw, _re.DOTALL)\n",
        "                if m:\n",
        "                    parsed = _json.loads(m.group(0))\n",
        "                    cov = str(parsed.get(\"covered\", \"no\")).lower()\n",
        "                    evidence = str(parsed.get(\"evidence\", \"\")).strip()\n",
        "                    if cov not in (\"yes\", \"no\"):\n",
        "                        cov = \"no\"\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            if cov == \"yes\":\n",
        "                covered += 1\n",
        "\n",
        "            details.append({\n",
        "                \"objective\": obj,\n",
        "                \"covered\": cov,\n",
        "                \"evidence\": evidence\n",
        "            })\n",
        "\n",
        "        total = len(objectives) or 1\n",
        "        base_score = round((covered / total) * 5)\n",
        "        score = min(5, max(1, base_score))\n",
        "\n",
        "        meta = {\n",
        "            \"covered_count\": covered,\n",
        "            \"total\": total,\n",
        "            \"details\": details\n",
        "        }\n",
        "        return score, meta\n",
        "\n",
        "    score, score_meta = score_ctx_objectives_only(context)\n",
        "\n",
        "    if score <= 2 and not refetch:\n",
        "        print(\"Low relevance detected. Refetching...\")\n",
        "        query = f\"{cp.topic} for beginners; \" + \"; \".join(cp.objectives)\n",
        "        results = search_tavily(query=query)\n",
        "\n",
        "        new_context = \"\"\n",
        "        for item in results:\n",
        "            c = item.get(\"content\")\n",
        "            if c:\n",
        "                new_context += c + \"\\n\"\n",
        "\n",
        "        if new_context.strip():\n",
        "            new_context = summarize_text(new_context)\n",
        "\n",
        "        score, score_meta = score_ctx_objectives_only(new_context)\n",
        "        context = new_context\n",
        "        refetch = True\n",
        "\n",
        "    state = dict(state)\n",
        "    state[\"gathered_context\"] = context\n",
        "    state[\"relevance_score_model\"] = score\n",
        "    state[\"refetch_attempted\"] = refetch\n",
        "    state[\"score_meta\"] = score_meta\n",
        "\n",
        "    if refetch and \"web_search\" not in state[\"context_sources\"]:\n",
        "        state[\"context_sources\"].append(\"web_search\")\n",
        "\n",
        "    print(f\"Score for {cp.id}: {score} ({score_meta['covered_count']}/{score_meta['total']} objectives covered)\")\n",
        "    for d in score_meta[\"details\"]:\n",
        "        print(f\" - Obj: {d['objective'][:60]}... => {d['covered']} | evidence: {clean_evidence(d['evidence'])[:140]}\")\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "# Milestone 2: Context processing (chunking) + build temp vector store\n",
        "\n",
        "@traceable(name=\"process_context_node\")\n",
        "def process_context(state: AgentState) -> AgentState:\n",
        "    context = (state.get(\"gathered_context\") or \"\").strip()\n",
        "    chunks: List[str] = []\n",
        "\n",
        "    if context:\n",
        "        start = 0\n",
        "        text_len = len(context)\n",
        "\n",
        "        while start < text_len:\n",
        "            end = start + CHUNK_SIZE\n",
        "            chunk = context[start:end].strip()\n",
        "\n",
        "            if len(chunk) >= MIN_CHUNK_LENGTH:\n",
        "                chunks.append(chunk)\n",
        "\n",
        "            # move forward with overlap\n",
        "            start = end - CHUNK_OVERLAP\n",
        "            if start < 0:\n",
        "                start = 0\n",
        "\n",
        "            # stop infinite loop\n",
        "            if start >= text_len:\n",
        "                break\n",
        "\n",
        "    # Fallback safety\n",
        "    if not chunks and context:\n",
        "        chunks = [context]\n",
        "\n",
        "    state = dict(state)\n",
        "    state[\"processed_chunks\"] = chunks\n",
        "\n",
        "    # Build temporary vector store (Milestone 2 requirement)\n",
        "    store = build_temp_vector_store(chunks)\n",
        "    state[\"temp_vector_store\"] = store\n",
        "\n",
        "    # Debug confirmation\n",
        "    if _EMBEDDING_DEBUG:\n",
        "        print(f\"[Chunking] Produced {len(chunks)} chunks \"\n",
        "              f\"(size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})\")\n",
        "        embedding_debug_print(store, label=f\"cp={state.get('cp_id')}\")\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "@traceable(name=\"generate_questions_node\")\n",
        "def generate_questions(state: AgentState) -> AgentState:\n",
        "    import time, random\n",
        "\n",
        "    cp = state[\"checkpoint\"]\n",
        "    chunks = state.get(\"processed_chunks\") or []\n",
        "    store = state.get(\"temp_vector_store\") or {\n",
        "        \"chunks\": chunks,\n",
        "        \"vectors\": None,\n",
        "        \"meta\": {\"embeddings_used\": False},\n",
        "    }\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Attempt-aware variation\n",
        "    # -------------------------------------------------\n",
        "    attempt = state.get(\"feynman_rounds\", 0)\n",
        "\n",
        "    QUESTION_STYLES = [\n",
        "        \"role\",\n",
        "        \"process\",\n",
        "        \"why\",\n",
        "        \"analogy\",\n",
        "        \"application\",\n",
        "    ]\n",
        "    style = QUESTION_STYLES[attempt % len(QUESTION_STYLES)]\n",
        "\n",
        "    # üî• TRUE ENTROPY\n",
        "    nonce = f\"{attempt}-{int(time.time()*1000)}-{random.randint(1000,9999)}\"\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Weak concepts\n",
        "    # -------------------------------------------------\n",
        "    focus_concepts = state.get(\"focus_concepts\") or cp.objectives\n",
        "    query = \" \".join(focus_concepts)\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Retrieve top-k chunks\n",
        "    # -------------------------------------------------\n",
        "    focus_for_generation = \"\"\n",
        "    if chunks:\n",
        "        top_chunks = retrieve_top_k(store, query=query, k=3)\n",
        "        focus_for_generation = \"\\n\\n\".join(top_chunks)\n",
        "\n",
        "    base_context = (focus_for_generation or \"\\n\\n\".join(chunks))[:3000]\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # LLM prompt (18 WORD LIMIT)\n",
        "    # -------------------------------------------------\n",
        "    prompt = f\"\"\"\n",
        "You are an adaptive AI tutor.\n",
        "\n",
        "SYSTEM NONCE: {nonce}\n",
        "\n",
        "ATTEMPT NUMBER: {attempt}\n",
        "QUESTION STYLE: {style}\n",
        "\n",
        "Generate EXACTLY 3 SHORT questions.\n",
        "\n",
        "STRICT FORMAT RULES:\n",
        "- ONE sentence only\n",
        "- MAXIMUM 18 words ‚¨ÖÔ∏è UPDATED\n",
        "- NO commas\n",
        "- NO chained clauses\n",
        "- MUST end with ?\n",
        "- SIMPLE beginner language\n",
        "- Focus ONLY on weak concepts\n",
        "- DIFFERENT from previous attempts\n",
        "- DO NOT start with: \"What is\", \"Define\", \"Explain\"\n",
        "\n",
        "WEAK CONCEPTS:\n",
        "{json.dumps(focus_concepts, ensure_ascii=False)}\n",
        "\n",
        "Return ONLY valid JSON:\n",
        "{{ \"questions\": [\"Q1\", \"Q2\", \"Q3\"] }}\n",
        "\"\"\"\n",
        "\n",
        "    raw = groq_chat(prompt, use_cache=False).strip()\n",
        "\n",
        "    questions = []\n",
        "    try:\n",
        "        m = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "        if m:\n",
        "            data = json.loads(m.group(0))\n",
        "            qlist = data.get(\"questions\", [])\n",
        "            if isinstance(qlist, list):\n",
        "                questions = [q.strip() for q in qlist if isinstance(q, str)]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Final safety trimming (18 words)\n",
        "    # -------------------------------------------------\n",
        "    clean_questions = []\n",
        "    for q in questions:\n",
        "        q = re.sub(r\",.*\", \"\", q)\n",
        "        q = \" \".join(q.split()[:18])        # ‚¨ÖÔ∏è UPDATED\n",
        "        if not q.endswith(\"?\"):\n",
        "            q += \"?\"\n",
        "        clean_questions.append(q)\n",
        "\n",
        "    state = dict(state)\n",
        "    state[\"questions\"] = clean_questions[:3]\n",
        "\n",
        "    print(f\"[Question Generation] attempt={attempt}, style={style}, nonce={nonce}\")\n",
        "    for q in state[\"questions\"]:\n",
        "        print(\" -\", q)\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "# Understanding verification (scoring)\n",
        "\n",
        "@traceable(name=\"verify_understanding_node\")\n",
        "def verify_understanding(state: AgentState) -> AgentState:\n",
        "\n",
        "    cp = state[\"checkpoint\"]\n",
        "    questions = state.get(\"questions\") or []\n",
        "    answers = state.get(\"learner_answers\") or []\n",
        "    store = state.get(\"temp_vector_store\") or {\n",
        "        \"chunks\": state.get(\"processed_chunks\", []),\n",
        "        \"vectors\": None,\n",
        "        \"meta\": {\"embeddings_used\": False},\n",
        "    }\n",
        "\n",
        "    if not questions or not answers:\n",
        "        print(f\"No questions or learner answers for {cp.id}; skipping verification.\")\n",
        "        state[\"score_percent\"] = None\n",
        "        state[\"pass_threshold_met\"] = None\n",
        "        return state\n",
        "\n",
        "    n = min(len(questions), len(answers))\n",
        "    if n == 0:\n",
        "        state[\"score_percent\"] = None\n",
        "        state[\"pass_threshold_met\"] = None\n",
        "        return state\n",
        "\n",
        "    scores = []\n",
        "    question_scores = []\n",
        "    failed_objectives = set()   # ‚úÖ NEW\n",
        "\n",
        "    def meaningful_word_count(text: str) -> int:\n",
        "        words = re.findall(r\"\\w+\", text or \"\")\n",
        "        return sum(1 for w in words if len(w) > 2)\n",
        "\n",
        "    for i in range(n):\n",
        "        q = questions[i]\n",
        "        a = answers[i] or \"\"\n",
        "\n",
        "        if meaningful_word_count(a) < 3:\n",
        "            score_val = 0\n",
        "            scores.append(score_val)\n",
        "            question_scores.append({\n",
        "               \"question\": q,\n",
        "                \"answer\": a,\n",
        "                \"score\": score_val\n",
        "        })\n",
        "\n",
        "            # map failure to objectives\n",
        "            for obj in cp.objectives:\n",
        "                if any(w.lower() in q.lower() for w in obj.split()):\n",
        "                    failed_objectives.add(obj)\n",
        "\n",
        "            print(f\"[Verify] {cp.id} Q{i+1} ‚Üí score 0\")\n",
        "            continue\n",
        "\n",
        "        top_chunks = retrieve_top_k(store, query=q, k=3)\n",
        "        context_for_grading = \"\\n\\n\".join(top_chunks)[:4000]\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "Return ONLY JSON.\n",
        "\n",
        "CONTEXT:\n",
        "\\\"\\\"\\\"{context_for_grading}\\\"\\\"\\\"\n",
        "\n",
        "QUESTION:\n",
        "{q}\n",
        "\n",
        "LEARNER ANSWER:\n",
        "{a}\n",
        "\n",
        "Rules:\n",
        "- No understanding ‚Üí 0‚Äì30\n",
        "- Partial ‚Üí 30‚Äì70\n",
        "- Mostly correct ‚Üí 70‚Äì100\n",
        "\n",
        "Return:\n",
        "{{ \"score\": <0-100> }}\n",
        "\"\"\"\n",
        "        raw = groq_chat(prompt).strip()\n",
        "\n",
        "        score_val = 0\n",
        "        try:\n",
        "            score_val = int(json.loads(re.search(r\"\\{.*\\}\", raw).group())[\"score\"])\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        score_val = max(0, min(100, score_val))\n",
        "        scores.append(score_val)\n",
        "        question_scores.append({\n",
        "            \"question\": q,\n",
        "            \"answer\": a,\n",
        "            \"score\": score_val\n",
        "        })\n",
        "\n",
        "\n",
        "        if score_val < 70:\n",
        "            for obj in cp.objectives:\n",
        "                if any(w.lower() in q.lower() for w in obj.split()):\n",
        "                    failed_objectives.add(obj)\n",
        "\n",
        "        print(f\"[Verify] {cp.id} Q{i+1} score: {score_val}\")\n",
        "\n",
        "    avg_score = sum(scores) / len(scores)\n",
        "    passed = avg_score >= 70\n",
        "\n",
        "    state[\"score_percent\"] = avg_score\n",
        "    state[\"pass_threshold_met\"] = passed\n",
        "    state[\"question_scores\"] = question_scores\n",
        "\n",
        "    # ‚úÖ STORE LEARNER-SPECIFIC WEAK CONCEPTS\n",
        "    state[\"failed_objectives\"] = list(failed_objectives)\n",
        "\n",
        "    if failed_objectives:\n",
        "        print(\"\\nüîç Learner weak concepts detected:\")\n",
        "        for o in failed_objectives:\n",
        "            print(\" -\", o)\n",
        "\n",
        "    print(f\"\\nOverall quiz score for {cp.id}: {avg_score:.1f}%\")\n",
        "\n",
        "    return state\n",
        "\n",
        "@traceable(name=\"feynman_node\")\n",
        "def feynman_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Adaptive Feynman Teaching:\n",
        "    - Teach ONLY what learner failed\n",
        "    - Explanation changes every round\n",
        "    \"\"\"\n",
        "\n",
        "    cp = state[\"checkpoint\"]\n",
        "\n",
        "    # ‚úÖ USE LEARNER FAILURES\n",
        "    weak_concepts = state.get(\"failed_objectives\") or []\n",
        "\n",
        "    if not weak_concepts:\n",
        "        weak_concepts = cp.objectives  # safety fallback\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a teacher using the Feynman Technique.\n",
        "\n",
        "The learner FAILED to understand the following concepts:\n",
        "{json.dumps(weak_concepts)}\n",
        "\n",
        "Explain them:\n",
        "- In very simple language\n",
        "- With real-life analogies\n",
        "- Step-by-step\n",
        "- Assume ZERO prior knowledge\n",
        "\n",
        "Make it different from previous explanations.\n",
        "\"\"\"\n",
        "\n",
        "    explanation = groq_chat(prompt, use_cache=False)\n",
        "\n",
        "\n",
        "    state[\"feynman_explanation\"] = explanation\n",
        "    state[\"focus_concepts\"] = weak_concepts\n",
        "    state[\"feynman_rounds\"] += 1\n",
        "\n",
        "    # üîÅ Inject explanation so next questions CHANGE\n",
        "    state[\"gathered_context\"] += (\n",
        "        \"\\n\\n### Simplified Explanation (Feynman)\\n\" + explanation\n",
        "    )\n",
        "\n",
        "    # reset assessment state\n",
        "    state[\"processed_chunks\"] = []\n",
        "    state[\"temp_vector_store\"] = None\n",
        "    state[\"learner_answers\"] = []\n",
        "    state[\"score_percent\"] = None\n",
        "    state[\"pass_threshold_met\"] = None\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üß† FEYNMAN TEACHING MODE\")\n",
        "    print(\"Weak concepts:\", weak_concepts)\n",
        "    print(\"=\" * 80)\n",
        "    print(explanation)\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def route_after_verification(state: AgentState) -> str:\n",
        "    score = state.get(\"score_percent\") or 0.0\n",
        "    attempts = state.get(\"feynman_rounds\", 0)\n",
        "\n",
        "    # Pass ‚Üí finish\n",
        "    if score >= 70:\n",
        "        return \"pass\"\n",
        "\n",
        "    # Fail but allow reteaching (max 2 loops)\n",
        "    if attempts < 2:\n",
        "        return \"feynman\"\n",
        "\n",
        "    # Too many failures ‚Üí stop\n",
        "    return \"pass\"\n",
        "\n",
        "def run_checkpoint_with_answers(cp_id: str, answers: List[str], prev_state=None):\n",
        "    \"\"\"\n",
        "    Runs ONE checkpoint with provided answers.\n",
        "    Used by Streamlit UI.\n",
        "    \"\"\"\n",
        "\n",
        "    graph = build_graph()\n",
        "\n",
        "    if prev_state is None:\n",
        "        state: AgentState = {\n",
        "            \"cp_id\": cp_id,\n",
        "            \"checkpoint\": None,\n",
        "            \"user_notes\": \"\",\n",
        "            \"user_pdfs\": [],\n",
        "            \"gathered_context\": \"\",\n",
        "            \"context_sources\": [],\n",
        "            \"relevance_score_model\": None,\n",
        "            \"refetch_attempted\": False,\n",
        "            \"score_meta\": None,\n",
        "            \"processed_chunks\": [],\n",
        "            \"questions\": [],\n",
        "            \"learner_answers\": [],\n",
        "            \"score_percent\": None,\n",
        "            \"pass_threshold_met\": None,\n",
        "            \"temp_vector_store\": None,\n",
        "            \"feynman_explanation\": None,\n",
        "            \"feynman_rounds\": 0,\n",
        "            \"focus_concepts\": None,\n",
        "        }\n",
        "    else:\n",
        "        state = prev_state\n",
        "\n",
        "    # Inject learner answers\n",
        "    state[\"learner_answers\"] = answers\n",
        "\n",
        "    final_state = graph.invoke(state)\n",
        "    return final_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_graph():\n",
        "    g = StateGraph(AgentState)\n",
        "\n",
        "    g.add_node(\"start_checkpoint\", start_checkpoint)\n",
        "    g.add_node(\"gather_context\", gather_context)\n",
        "    g.add_node(\"validate_context\", validate_context)\n",
        "    g.add_node(\"process_context\", process_context)\n",
        "    g.add_node(\"generate_questions\", generate_questions)\n",
        "    g.add_node(\"verify_understanding\", verify_understanding)\n",
        "    g.add_node(\"feynman_node\", feynman_node)\n",
        "\n",
        "    g.set_entry_point(\"start_checkpoint\")\n",
        "\n",
        "    g.add_edge(\"start_checkpoint\", \"gather_context\")\n",
        "    g.add_edge(\"gather_context\", \"validate_context\")\n",
        "    g.add_edge(\"validate_context\", \"process_context\")\n",
        "    g.add_edge(\"process_context\", \"generate_questions\")\n",
        "    g.add_edge(\"generate_questions\", \"verify_understanding\")\n",
        "\n",
        "    # üîÅ Decision point\n",
        "    g.add_conditional_edges(\n",
        "        \"verify_understanding\",\n",
        "        route_after_verification,\n",
        "        {\n",
        "            \"pass\": END,\n",
        "            \"feynman\": \"feynman_node\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # üîÅ Loop back after teaching\n",
        "    g.add_edge(\"feynman_node\", \"generate_questions\")\n",
        "\n",
        "    return g.compile()\n",
        "\n",
        "\n",
        "\n",
        "graph = build_graph()\n",
        "\n",
        "\n",
        "\n",
        "# Helper: read multi-line input (end with an 'END' line)\n",
        "\n",
        "def read_multiline(prompt_msg: str) -> str:\n",
        "    print(prompt_msg)\n",
        "    print(\"Enter/Paste your text. End with a single line containing only: END\")\n",
        "    lines = []\n",
        "    while True:\n",
        "        try:\n",
        "            line = input()\n",
        "        except EOFError:\n",
        "            break\n",
        "        if line.strip() == \"END\":\n",
        "            break\n",
        "        lines.append(line)\n",
        "    return \"\\n\".join(lines).strip()\n",
        "\n",
        "\n",
        "def all_objectives_mastered(state: AgentState) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True only if ALL objectives are marked as covered=yes\n",
        "    \"\"\"\n",
        "    meta = state.get(\"score_meta\")\n",
        "    if not meta:\n",
        "        return False\n",
        "\n",
        "    return all(\n",
        "        item.get(\"covered\") == \"yes\"\n",
        "        for item in meta.get(\"details\", [])\n",
        "    )\n",
        "\n",
        "def run_single_checkpoint_interactive(cp_id: str) -> dict:\n",
        "    cp = get_checkpoint_by_id(cp_id)\n",
        "    print(f\"\\n--- Checkpoint {cp.id}: {cp.topic} ---\")\n",
        "\n",
        "    notes = read_multiline(\"Provide user notes for this checkpoint (or leave blank and type END):\")\n",
        "    pdfs_input = input(\"Enter comma-separated PDF paths for this checkpoint (or leave blank): \").strip()\n",
        "    pdfs = [p.strip() for p in pdfs_input.split(\",\") if p.strip()]\n",
        "\n",
        "    # ---------------------------\n",
        "    # INITIAL STATE\n",
        "    # ---------------------------\n",
        "    state: AgentState = {\n",
        "        \"cp_id\": cp_id,\n",
        "        \"checkpoint\": None,\n",
        "        \"user_notes\": notes,\n",
        "        \"user_pdfs\": pdfs,\n",
        "        \"gathered_context\": \"\",\n",
        "        \"context_sources\": [],\n",
        "        \"relevance_score_model\": None,\n",
        "        \"refetch_attempted\": False,\n",
        "        \"score_meta\": None,\n",
        "        \"processed_chunks\": [],\n",
        "        \"questions\": [],\n",
        "        \"learner_answers\": [],\n",
        "        \"score_percent\": None,\n",
        "        \"pass_threshold_met\": None,\n",
        "        \"temp_vector_store\": None,\n",
        "        \"feynman_explanation\": None,\n",
        "        \"feynman_rounds\": 0,\n",
        "        \"focus_concepts\": None,\n",
        "    }\n",
        "\n",
        "    # ---------------------------\n",
        "    # INITIAL PIPELINE (RUN ONCE)\n",
        "    # ---------------------------\n",
        "    state = start_checkpoint(state)\n",
        "    state = gather_context(state)\n",
        "    state = validate_context(state)\n",
        "    state = process_context(state)\n",
        "\n",
        "    MAX_REASSESSMENTS = 3\n",
        "\n",
        "    # ===========================\n",
        "    # üîÅ TEACH‚ÄìASSESS LOOP\n",
        "    # ===========================\n",
        "    while True:\n",
        "\n",
        "        # 1Ô∏è‚É£ Generate questions\n",
        "        state = generate_questions(state)\n",
        "\n",
        "        print(\"\\nGenerated Questions:\")\n",
        "        for i, q in enumerate(state[\"questions\"], 1):\n",
        "            print(f\"{i}. {q}\")\n",
        "\n",
        "        # 2Ô∏è‚É£ Collect answers\n",
        "        answers = []\n",
        "        for i, q in enumerate(state[\"questions\"], 1):\n",
        "            ans = read_multiline(f\"\\nAnswer for Q{i}: {q}\")\n",
        "            answers.append(ans)\n",
        "\n",
        "        state[\"learner_answers\"] = answers\n",
        "\n",
        "        # 3Ô∏è‚É£ Verify understanding\n",
        "        state = verify_understanding(state)\n",
        "        state = validate_context(state)\n",
        "        # 4Ô∏è‚É£ STRICT EXIT CONDITION\n",
        "        if state.get(\"pass_threshold_met\") and all_objectives_mastered(state):\n",
        "            print(\"\\n‚úÖ Passed! All concepts mastered.\")\n",
        "            break\n",
        "\n",
        "        # 5Ô∏è‚É£ Stop if max reteaching reached\n",
        "        if state.get(\"feynman_rounds\", 0) >= MAX_REASSESSMENTS:\n",
        "            print(\"\\n‚õî Maximum re-teaching attempts reached.\")\n",
        "            break\n",
        "\n",
        "        # 6Ô∏è‚É£ Feynman remediation\n",
        "        print(\"\\n‚ùå Score below threshold ‚Äî entering Feynman teaching mode\\n\")\n",
        "        state = feynman_node(state)\n",
        "\n",
        "        # 7Ô∏è‚É£ Re-process context so explanation affects next questions\n",
        "        state = process_context(state)\n",
        "\n",
        "        print(\"\\nüîÅ Re-attempting questions...\\n\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # FINAL RESULT\n",
        "    # ---------------------------\n",
        "    return {\n",
        "        \"cp_id\": cp_id,\n",
        "        \"topic\": cp.topic,\n",
        "        \"context_score\": state.get(\"relevance_score_model\"),\n",
        "        \"quiz_score\": state.get(\"score_percent\"),\n",
        "        \"passed\": state.get(\"pass_threshold_met\") and all_objectives_mastered(state),\n",
        "        \"sources\": state.get(\"context_sources\"),\n",
        "        \"questions\": state.get(\"questions\"),\n",
        "        \"answers\": state.get(\"learner_answers\"),\n",
        "    }\n",
        "\n",
        "\n",
        "# Evaluation suite (automated tests for Q relevance & scoring)\n",
        "\n",
        "def _generate_good_answer_for_question(context: str, question: str) -> str:\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Use the CONTEXT below to write a concise, correct, and focused answer to the QUESTION.\n",
        "Answer must be at least 25 words (to avoid short-answer penalties) and at most 80 words.\n",
        "Keep it factual and directly relevant to the question.\n",
        "\n",
        "CONTEXT:\n",
        "\\\"\\\"\\\"{context[:3500]}\\\"\\\"\\\"\\n\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    raw = groq_chat(prompt)\n",
        "    txt = raw.strip()\n",
        "    words = re.findall(r\"\\w+\", txt)\n",
        "    if len([w for w in words if len(w) > 2]) < 15:\n",
        "        fallback = f\"Provide a direct, explanatory answer (>=25 words) to: {question}\\nUsing: {context[:800]}\"\n",
        "        raw2 = groq_chat(fallback)\n",
        "        txt = raw2.strip() or txt\n",
        "    return txt\n",
        "\n",
        "\n",
        "def run_evaluation_suite():\n",
        "    \"\"\"\n",
        "    Runs automated evaluation across all CHECKPOINTS.\n",
        "    For each checkpoint:\n",
        "      - gather/process/generate questions\n",
        "      - create 'good' answers via LLM (ensured to be long enough)\n",
        "      - create 'bad' answers (short/off-topic)\n",
        "      - run verify_understanding for both sets and record metrics\n",
        "    \"\"\"\n",
        "    overall = []\n",
        "    print(\"Running automated evaluation suite for all checkpoints...\\n\")\n",
        "    for cp in CHECKPOINTS:\n",
        "        # initialize state\n",
        "        state: AgentState = {\n",
        "            \"cp_id\": cp.id,\n",
        "            \"checkpoint\": None,\n",
        "            \"user_notes\": \"\",  # no notes; will use web search fallback\n",
        "            \"user_pdfs\": [],\n",
        "            \"gathered_context\": \"\",\n",
        "            \"context_sources\": [],\n",
        "            \"relevance_score_model\": None,\n",
        "            \"refetch_attempted\": False,\n",
        "            \"score_meta\": None,\n",
        "            \"processed_chunks\": [],\n",
        "            \"questions\": [],\n",
        "            \"learner_answers\": [],\n",
        "            \"score_percent\": None,\n",
        "            \"pass_threshold_met\": None,\n",
        "            \"temp_vector_store\": None,\n",
        "        }\n",
        "\n",
        "        state = start_checkpoint(state)\n",
        "        state = gather_context(state)\n",
        "        state = validate_context(state)\n",
        "        state = process_context(state)\n",
        "        state = generate_questions(state)\n",
        "\n",
        "        questions = state.get(\"questions\") or []\n",
        "        context_text = state.get(\"gathered_context\") or \"\\n\".join(state.get(\"processed_chunks\", [])) or \"\"\n",
        "        store = state.get(\"temp_vector_store\") or {\"chunks\": [], \"vectors\": None, \"meta\": {\"embeddings_used\": False}}\n",
        "\n",
        "        # Double-confirm embedding usage for this checkpoint\n",
        "        emb_used = bool(store.get(\"vectors\") is not None)\n",
        "        print(f\"[Eval] Checkpoint {cp.id} embeddings_used = {emb_used}\")\n",
        "        if emb_used:\n",
        "            try:\n",
        "                embedding_debug_print(store, label=f\"eval-cp={cp.id}\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Build 'good' answers using the LLM (ensured to be long enough)\n",
        "        good_answers = []\n",
        "        for q in questions:\n",
        "            ans = _generate_good_answer_for_question(context_text, q)\n",
        "            if len(re.findall(r\"\\w+\", ans)) < 30:\n",
        "                ans = ans + \" \" + (\"This answer expands on the main points to ensure full coverage of the objective. \" * 2)\n",
        "            good_answers.append(ans)\n",
        "\n",
        "        # Build 'bad' answers (short/off-topic)\n",
        "        bad_answers = [\"I don't know.\" for _ in questions]\n",
        "\n",
        "        # Evaluate good answers\n",
        "        state_good = dict(state)\n",
        "        state_good[\"learner_answers\"] = good_answers\n",
        "        state_good = verify_understanding(state_good)\n",
        "        good_score = state_good.get(\"score_percent\") or 0.0\n",
        "        good_pass = state_good.get(\"pass_threshold_met\") or False\n",
        "\n",
        "        # Evaluate bad answers\n",
        "        state_bad = dict(state)\n",
        "        state_bad[\"learner_answers\"] = bad_answers\n",
        "        state_bad = verify_understanding(state_bad)\n",
        "        bad_score = state_bad.get(\"score_percent\") or 0.0\n",
        "        bad_pass = state_bad.get(\"pass_threshold_met\") or False\n",
        "\n",
        "        q_rel = 1.0 if questions else 0.0\n",
        "\n",
        "        overall.append({\n",
        "            \"cp_id\": cp.id,\n",
        "            \"topic\": cp.topic,\n",
        "            \"num_questions\": len(questions),\n",
        "            \"q_rel\": q_rel,\n",
        "            \"context_score\": state.get(\"relevance_score_model\"),\n",
        "            \"good_score\": good_score,\n",
        "            \"good_pass\": good_pass,\n",
        "            \"bad_score\": bad_score,\n",
        "            \"bad_pass\": bad_pass,\n",
        "            \"embeddings_used\": emb_used,\n",
        "        })\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n=== Evaluation Summary ===\\n\")\n",
        "    total_q_rel = 0.0\n",
        "    good_pass_count = 0\n",
        "    bad_fail_count = 0\n",
        "    emb_used_count = 0\n",
        "    for r in overall:\n",
        "        print(f\"Checkpoint: {r['cp_id']} - {r['topic']}\")\n",
        "        print(f\"  Questions: {r['num_questions']}, Q-rel: {r['q_rel']:.2f}\")\n",
        "        print(f\"  Context score (1-5): {r['context_score']}\")\n",
        "        print(f\"  Good answers -> score: {r['good_score']:.1f}%, pass: {r['good_pass']}\")\n",
        "        print(f\"  Bad answers  -> score: {r['bad_score']:.1f}%, pass: {r['bad_pass']}\")\n",
        "        print(f\"  Embeddings used: {r.get('embeddings_used')}\")\n",
        "        print()\n",
        "        total_q_rel += r['q_rel']\n",
        "        if r['good_pass']:\n",
        "            good_pass_count += 1\n",
        "        if not r['bad_pass']:\n",
        "            bad_fail_count += 1\n",
        "        if r.get('embeddings_used'):\n",
        "            emb_used_count += 1\n",
        "\n",
        "    n = len(overall) or 1\n",
        "    print(\"--- Overall Metrics ---\")\n",
        "    print(f\"Average question relevance (fraction): {total_q_rel / n:.3f}\")\n",
        "    print(f\"Good answers pass-rate (should be high): {good_pass_count / n * 100:.1f}%\")\n",
        "    print(f\"Bad answers fail-rate (should be high): {bad_fail_count / n * 100:.1f}%\")\n",
        "    print(f\"Embeddings used in checkpoints: {emb_used_count}/{n}\\n\")\n",
        "\n",
        "    return overall\n",
        "\n",
        "\n",
        "\n",
        "# Interactive run for multiple checkpoints\n",
        "\n",
        "def interactive_run():\n",
        "    print(\"Interactive Milestone 2 runner.\")\n",
        "    print(\"Available checkpoints:\")\n",
        "    for cp in CHECKPOINTS:\n",
        "        print(f\" - {cp.id}: {cp.topic}\")\n",
        "\n",
        "    chosen = input(\"Enter comma-separated checkpoint ids to run (or 'all'): \").strip()\n",
        "    if chosen.lower() == \"all\" or not chosen:\n",
        "        ids = [cp.id for cp in CHECKPOINTS]\n",
        "    else:\n",
        "        ids = [c.strip() for c in chosen.split(\",\") if c.strip()]\n",
        "\n",
        "    results = []\n",
        "    for cp_id in ids:\n",
        "        try:\n",
        "            res = run_single_checkpoint_interactive(cp_id)\n",
        "            results.append(res)\n",
        "        except Exception as e:\n",
        "            print(f\"Error running {cp_id}: {e}\")\n",
        "\n",
        "    # Build markdown summary table\n",
        "    lines = []\n",
        "    lines.append(\"### Summary Table\\n\")\n",
        "    lines.append(\"| Topic | Objectives | Sources | Context Score (1‚Äì5) | Quiz Score (%) | Pass (>=70%) |\")\n",
        "    lines.append(\"|-------|------------|---------|---------------------|----------------|--------------|\")\n",
        "    for r in results:\n",
        "        cp = get_checkpoint_by_id(r[\"cp_id\"])\n",
        "        obj_list = [f\"- {o}\" for o in cp.objectives]\n",
        "        objectives_str = \"<br>\".join(obj_list)\n",
        "        sources_str = \", \".join(r[\"sources\"]) if r[\"sources\"] else \"None\"\n",
        "        context_score = r[\"context_score\"]\n",
        "        quiz_score = r[\"quiz_score\"]\n",
        "        quiz_display = \"-\" if quiz_score is None else f\"{quiz_score:.1f}\"\n",
        "        passed = r[\"passed\"]\n",
        "        passed_str = \"‚úÖ\" if passed else (\"‚ùå\" if passed is not None else \"-\")\n",
        "        lines.append(f\"| {r['topic']} | {objectives_str} | {sources_str} | {context_score} | {quiz_display} | {passed_str} |\")\n",
        "\n",
        "    table_md = \"\\n\".join(lines)\n",
        "    try:\n",
        "        display(Markdown(table_md))\n",
        "    except Exception:\n",
        "        print(table_md)\n",
        "\n",
        "    # Save ALL raw LLM prompt+response logs into ONE PDF\n",
        "    save_all_raw_to_one_pdf(\"all_llm_raw_output.pdf\")\n",
        "\n",
        "\n",
        "# Main\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Milestone 1 & 2 runner\")\n",
        "    print(\"Options:\\n  1) interactive run (generate questions & answer interactively)\\n  2) run evaluation suite (automated tests for Q relevance & scoring)\")\n",
        "    choice = input(\"Enter 1 or 2 (default 1): \").strip() or \"1\"\n",
        "    if choice == \"2\":\n",
        "        run_evaluation_suite()\n",
        "    else:\n",
        "        interactive_run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcwT_uQERTqq",
        "outputId": "8c5e661a-5fe1-4ced-b780-9176db5728b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting backend.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py"
      ],
      "metadata": {
        "id": "AOSycZrLf0-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from backend import CHECKPOINTS, get_checkpoint_by_id, run_checkpoint_with_answers\n",
        "\n",
        "st.set_page_config(page_title=\"Adaptive AI Tutor\", layout=\"wide\")\n",
        "\n",
        "# ======================================================\n",
        "# SESSION STATE INITIALIZATION\n",
        "# ======================================================\n",
        "if \"phase\" not in st.session_state:\n",
        "    st.session_state.phase = \"context\"   # context | test | result\n",
        "\n",
        "if \"state\" not in st.session_state:\n",
        "    st.session_state.state = None\n",
        "\n",
        "if \"questions\" not in st.session_state:\n",
        "    st.session_state.questions = []\n",
        "\n",
        "if \"answers\" not in st.session_state:\n",
        "    st.session_state.answers = []\n",
        "\n",
        "if \"attempts_left\" not in st.session_state:\n",
        "    st.session_state.attempts_left = 3\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# ANSWER NORMALIZATION (CRITICAL FIX)\n",
        "# ======================================================\n",
        "def normalize_answer(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    return (\n",
        "        text.lower()\n",
        "        .replace(\"\\n\", \" \")\n",
        "        .replace(\".\", \"\")\n",
        "        .replace(\",\", \"\")\n",
        "        .replace(\"-\", \" \")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# SAFE INITIAL STATE (MATCHES BACKEND AgentState)\n",
        "# ======================================================\n",
        "def fresh_state(cp_id, user_notes):\n",
        "    return {\n",
        "        \"cp_id\": cp_id,\n",
        "        \"checkpoint\": None,\n",
        "        \"user_notes\": user_notes or \"\",\n",
        "        \"user_pdfs\": [],\n",
        "        \"gathered_context\": \"\",\n",
        "        \"context_sources\": [],\n",
        "        \"relevance_score_model\": None,\n",
        "        \"refetch_attempted\": False,\n",
        "        \"score_meta\": None,\n",
        "        \"processed_chunks\": [],\n",
        "        \"questions\": [],\n",
        "        \"learner_answers\": [],\n",
        "        \"score_percent\": None,\n",
        "        \"pass_threshold_met\": None,\n",
        "        \"temp_vector_store\": None,\n",
        "        \"feynman_explanation\": None,\n",
        "        \"feynman_rounds\": 0,\n",
        "        \"focus_concepts\": None,\n",
        "        \"failed_objectives\": [],\n",
        "    }\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# SIDEBAR\n",
        "# ======================================================\n",
        "st.sidebar.title(\"üß≠ Control\")\n",
        "\n",
        "cp_id = st.sidebar.selectbox(\n",
        "    \"Select Checkpoint\",\n",
        "    [c.id for c in CHECKPOINTS],\n",
        "    format_func=lambda x: f\"{x} ‚Äî {get_checkpoint_by_id(x).topic}\",\n",
        ")\n",
        "\n",
        "if st.sidebar.button(\"üîÑ Reset Session\"):\n",
        "    st.session_state.clear()\n",
        "    st.rerun()\n",
        "\n",
        "cp = get_checkpoint_by_id(cp_id)\n",
        "\n",
        "# ======================================================\n",
        "# HEADER\n",
        "# ======================================================\n",
        "st.title(\"üß† Adaptive AI Tutor (Feynman Learning)\")\n",
        "st.subheader(cp.topic)\n",
        "\n",
        "with st.expander(\"üéØ Learning Objectives\", expanded=True):\n",
        "    for obj in cp.objectives:\n",
        "        st.markdown(f\"- {obj}\")\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# PHASE 1 ‚Äî CONTEXT\n",
        "# ======================================================\n",
        "if st.session_state.phase == \"context\":\n",
        "\n",
        "    user_notes = st.text_area(\"Paste your notes (optional)\", height=200)\n",
        "\n",
        "    if st.button(\"üìö Load Content\"):\n",
        "        st.session_state.state = run_checkpoint_with_answers(\n",
        "            cp_id=cp_id,\n",
        "            answers=[\"_init_\"],  # dummy trigger\n",
        "            prev_state=fresh_state(cp_id, user_notes),\n",
        "        )\n",
        "        st.rerun()\n",
        "\n",
        "    if st.session_state.state and st.session_state.state.get(\"gathered_context\"):\n",
        "        st.markdown(\"### üìö Context Used by the System\")\n",
        "        st.text_area(\n",
        "            label=\"\",\n",
        "            value=st.session_state.state[\"gathered_context\"],\n",
        "            height=300,\n",
        "        )\n",
        "\n",
        "        if st.button(\"üìù Take Test\"):\n",
        "            st.session_state.questions = st.session_state.state.get(\"questions\", [])\n",
        "            st.session_state.answers = [\"\"] * len(st.session_state.questions)\n",
        "            st.session_state.phase = \"test\"\n",
        "            st.rerun()\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# PHASE 2 ‚Äî TEST\n",
        "# ======================================================\n",
        "if st.session_state.phase == \"test\":\n",
        "\n",
        "    st.markdown(\"## ‚ùì Answer the Questions\")\n",
        "\n",
        "    if len(st.session_state.answers) != len(st.session_state.questions):\n",
        "        st.session_state.answers = [\"\"] * len(st.session_state.questions)\n",
        "\n",
        "    for i, q in enumerate(st.session_state.questions):\n",
        "        st.session_state.answers[i] = st.text_area(\n",
        "            f\"Q{i+1}: {q}\",\n",
        "            value=st.session_state.answers[i],\n",
        "            key=f\"answer_q{i}\",\n",
        "            height=120,\n",
        "        )\n",
        "\n",
        "    if st.button(\"‚úÖ Submit Answers\"):\n",
        "        s = st.session_state.state\n",
        "\n",
        "        # üî• Normalize answers BEFORE evaluation\n",
        "        normalized_answers = [\n",
        "            normalize_answer(a) for a in st.session_state.answers\n",
        "        ]\n",
        "\n",
        "        s[\"learner_answers\"] = normalized_answers\n",
        "\n",
        "        s = run_checkpoint_with_answers(\n",
        "            cp_id=cp_id,\n",
        "            answers=normalized_answers,\n",
        "            prev_state=s,\n",
        "        )\n",
        "\n",
        "        st.session_state.state = s\n",
        "        st.session_state.phase = \"result\"\n",
        "\n",
        "        if not s.get(\"pass_threshold_met\"):\n",
        "            st.session_state.attempts_left -= 1\n",
        "\n",
        "        st.rerun()\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# PHASE 3 ‚Äî RESULT\n",
        "# ======================================================\n",
        "if st.session_state.phase == \"result\":\n",
        "\n",
        "    s = st.session_state.state\n",
        "    score = s.get(\"score_percent\")\n",
        "\n",
        "    st.markdown(\"## üìä Assessment Result\")\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "    col1.metric(\n",
        "        \"Score (%)\",\n",
        "        f\"{score:.1f}\" if isinstance(score, (int, float)) else \"‚Äî\",\n",
        "    )\n",
        "    col2.metric(\"Attempts Left\", st.session_state.attempts_left)\n",
        "\n",
        "    if s.get(\"pass_threshold_met\"):\n",
        "        st.success(\"üéâ Passed! You have mastered this checkpoint.\")\n",
        "        st.stop()\n",
        "\n",
        "    st.error(\"‚ùå Failed\")\n",
        "\n",
        "    st.markdown(\"## üß† Feynman Explanation\")\n",
        "    st.info(s.get(\"feynman_explanation\", \"No explanation available.\"))\n",
        "\n",
        "    if st.session_state.attempts_left > 0:\n",
        "        if st.button(\"üîÅ Re-take Test\"):\n",
        "\n",
        "            st.session_state.questions = []\n",
        "            st.session_state.answers = []\n",
        "\n",
        "            s[\"checkpoint\"] = None\n",
        "            s[\"questions\"] = []\n",
        "            s[\"processed_chunks\"] = []\n",
        "            s[\"temp_vector_store\"] = None\n",
        "            s[\"learner_answers\"] = []\n",
        "\n",
        "            s[\"focus_concepts\"] = s.get(\"failed_objectives\")\n",
        "\n",
        "            s = run_checkpoint_with_answers(\n",
        "                cp_id=cp_id,\n",
        "                answers=[\"_retry_\"],\n",
        "                prev_state=s,\n",
        "            )\n",
        "\n",
        "            st.session_state.state = s\n",
        "            st.session_state.questions = s.get(\"questions\", [])\n",
        "            st.session_state.phase = \"test\"\n",
        "            st.rerun()\n",
        "    else:\n",
        "        st.error(\"‚õî Maximum 3 attempts reached.\")\n"
      ],
      "metadata": {
        "id": "aqJy5F3lopcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd099be8-79c0-4367-f7fc-2bff39aa204c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run app.py \\\n",
        "  --server.port 8501 \\\n",
        "  --server.address 0.0.0.0 \\\n",
        "  > /content/streamlit.log 2>&1 &\n"
      ],
      "metadata": {
        "id": "1sB3Z1947Pre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i :8501\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZQlcskQiSJx",
        "outputId": "5055b504-37b6-446c-8336-03ada69c58b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND     PID USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\n",
            "streamlit 39438 root    6u  IPv4 1025233      0t0  TCP *:8501 (LISTEN)\n",
            "streamlit 39438 root   21u  IPv4 1349681      0t0  TCP localhost:8501->localhost:57768 (ESTABLISHED)\n",
            "ngrok     51683 root   14u  IPv4 1349680      0t0  TCP localhost:57768->localhost:8501 (ESTABLISHED)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n"
      ],
      "metadata": {
        "id": "WeceIQxqa3Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üåç Streamlit Public URL:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3D4-_mchvCQ",
        "outputId": "ec5ad15b-a0cc-4d28-bef7-e1ce14109b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåç Streamlit Public URL: NgrokTunnel: \"https://unflushed-hailee-unaccorded.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}